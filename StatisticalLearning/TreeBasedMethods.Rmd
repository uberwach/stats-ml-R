---
title: "Treebased_Methods"
author: "Gerrit"
date: "20.03.2015"
output: html_document
---

Decision Trees
==============

We will look at the `Carseats` data with the tree package of R.

```{r}
require(ISLR)
require(tree)
attach(Carseats)
```

```{r}
hist(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
```

Now fit a decision tree to this data.

```{r}
tree.carseats = tree(High ~. - Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

Or a detailed version of the tree could be printed.

```{r}
tree.carseats
```

Lets split the 400 observations into a training and a test set. Fit a new model on the train data.

```{r}
set.seed(1011)
train = sample(1:nrow(Carseats), 250)
tree.carseats = tree(High ~ . - Sales, Carseats, subset = train)

plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

And test the model on the test data:

```{r}
tree.pred = predict(tree.carseats, Carseats[-train,], type = "class")
with(Carseats[-train,], table(tree.pred, High))
```

Use CV to prune the tree.

```{r}
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
plot(cv.carseats)
```

```{r}
prune.carseats = prune.misclass(tree.carseats, best = 13)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

Evaluating the pruned tree on the test data.

```{r}
tree.pred = predict(prune.carseats, Carseats[-train,], type = "class")
with(Carseats[-train,], table(tree.pred, High))
```


Random Forest and Boosting
==========================

Random Forests
--------------

```{r}
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train = sample(1 : nrow(Boston), 300)
```

Estimate the median housing value "medv" variable with a random forest model.

```{r}
rf.boston = randomForest(medv ~ ., data = Boston, subset = train)
rf.boston
```

Four variables were tried each split. Since $p = 13$ it could be tried to set `mtry` for ever possible value and compare results.

```{r}
oob.err = double(13)
test.err = double(13)

for (mtry in 1:13) {
  fit = randomForest(medv ~ ., data = Boston, subset = train, mtry = mtry, ntree = 400)
  oob.err[mtry] = fit$mse[400]
  pred = predict(fit, Boston[-train,])
  test.err[mtry] = with(Boston[-train,], mean((medv-pred)^2))
  cat(mtry, " ")
}

matplot(1 : mtry, cbind(test.err, oob.err), pch = 19, col = c("red", "blue"), type = "b", ylab = "Mean Squared Error")
legend("topright", legend = c("OOB", "TEST", pch = 19, col = c ("red", "blue")))
```




Boosting
--------

Boosting builds lots of smaller trees. Each new tree tries to patch up deficiences of the current ensemble (unlike RF).

```{r}
require(gbm)
boost.boston = gbm(medv ~ ., data = Boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston, i = "lstat")
plot(boost.boston, i = "rm")
```

In boostom the number of trees is a tuning parameter, having a value too high may overfit the training data. Use CV to select the number of trees.

```{r}
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees = n.trees)
dim(predmat)
berr = with(Boston[-train,], apply( (predmat-medv)^2, 2, mean))
plot(n.trees, berr, pch = 19, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")
```

