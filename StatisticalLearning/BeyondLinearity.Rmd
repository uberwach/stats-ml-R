---
title: "Moving_Beyond_Linearity"
author: "Gerrit"
date: "12.03.2015"
output: html_document
---

Nonlinear Models
=======================

```{r}
require(ISLR)
attach(Wage)
```

Polynomials
-----------

Try to fit in a linear model on the single predictor age with polynomial of degree 4.

```{r}
fit = lm(wage ~ poly(age,4), data = Wage)
summary(fit)
```

Plot the fitted polynomial together with standard error bands (dashed lines) against the data in a scatter plot.

```{r fig.width = 7, fig.height = 6}
agelims = range(age)
age.grid = seq(from = agelims[1], to = agelims[2])
preds = predict(fit, newdata = list(age = age.grid), se = TRUE)
se.bands = cbind(preds$fit + 2 * preds$se, preds$fit - 2*preds$se)
plot(age, wage, col = "darkgrey")
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, col = "blue", lty = 2)
```

Another way of writing down the way to fit a linear model with the help of a polynomial of degree 4.

```{r}
fita = lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage)
summary(fita)
```

The summary actually looks different, this is because a different basis is used.

```{r}
plot(fitted(fit), fitted(fita))
```

It can be seen, however, that the quartic coefficient is not needed and both models are the same (just that the coefficients are given with respect to a different basis).

```{r}
fita = lm(wage ~ education, data = Wage)
fitb = lm(wage ~ education + age, data = Wage)
fitc = lm(wage ~ education + poly(age,2), data = Wage)
fitd = lm(wage ~ education + poly(age,3), data = Wage)
anova(fita, fitb, fitc, fitd)
```

### Polynomial logistic regression

Fitting a logistic regression model with binary response variable (code the big earners with an income of more than $250k).

```{r}
fit = glm(I(wage > 250) ~ poly(age,3), data = Wage, family = binomial)
summary(fit)
preds = predict(fit, list(age = age.grid), se = T)
se.bands = preds$fit + cbind(fit = 0, lower = -2*preds$se, upper = 2*preds$se)
se.bands[1:5,]
```

The computation were done on the logit scale, so the inverse logit has to be applied.

$$p = \frac{e^\eta}{1 + e^\eta}.$$

```{r}
prob.bands = exp(se.bands) / (1 + exp(se.bands))
matplot(age.grid, prob.bands, col = "blue", lwd = c(2,1,1), lty = c(1,2,2), type = "l", ylim = c(0,.1))
points(jitter(age), I(wage > 250) / 10, pch = "|", cex = 0.5)
```


Splines
-------

In the following cubic splines are explorer, a new package is needed.

```{r}
require(splines)

fit = lm(wage ~ bs(age, knots = c(25,40,60)), data = Wage)
plot(age, wage, col = "darkgrey")
lines(age.grid, predict(fit, list(age = age.grid)), col = "darkgreen", lwd = 2)
# plots the knots as vertical lines
abline(v = c(25,40,60), lty = 2, col = "darkgreen")

# select smoothing splines by degrees of freedom
fit = smooth.spline(age, wage, df = 16) # df = 16 is a lot..
lines(fit, col = "red", lwd = 2)

# use CV (LOOCV is used) to select smoothing parameter
fit = smooth.spline(age, wage, cv = TRUE)
lines(fit, col = "purple", lwd = 2)
```

The smoothing parameter chosen by using CV is around $6.8$ and much lower than we tried before.

Generalized Additive Models
---------------------------

So far models have been fitted on mostly single nonlinear terms. The package `gam` makes it easier to work with multiple nonlinear terms.

```{r fig.width=10, fig.height=5}
require(gam)
# s function means smoothing spline
gam1 = gam(wage ~ s(age,df=4) + s(year, df = 4) + education, data = Wage)
plot(gam1, se = T)
# classification
gam2 = gam(I(wage > 250) ~ s(age, df = 4) + s(year, df = 4) + education, family = binomial)
plot(gam2)
```

```{r}
gam2a = gam(I(wage > 250) ~ s(age, df = 4) + year + education, data = Wage, family = binomial)
anova(gam2a, gam2, test = "Chisq")
```

One feature of the `gam` package is that it knows how to plot in a nice way. Even for models fit by `lm` and `glm`

```{r fig.width=10, fig.height=5}
par(mrow = c(1,3))
lm1 = lm(wage ~ ns(age,df = 4) + ns(year, df = 4) + education, data = Wage)
plot.gam(lm1, se = T)
```

Homework
--------

```{r}
load("7.R.RData") # data set is x,y
plot(x,y)
```

```{r}
fit = lm(y ~ x)

```

A coefficient of $-0.67483$ for the linear term can be read off.

```{r}
fit2 = lm(y ~ I(x^2) + I(x) + 1)
summary(fit2)
```

A coefficient of around $77.71$ for the coefficient $\alpha$ of $x$ can be read off.
