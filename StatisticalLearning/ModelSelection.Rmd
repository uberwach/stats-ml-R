---
title: "Model Selection"
author: "Gerrit"
date: "10.03.2015"
output: html_document
---

Model Selection
===============

```{r}
library(ISLR)
summary(Hitters)
```

Clean the data set:

```{r}
Hitters = na.omit(Hitters) # removes na entries
with(Hitters, sum(is.na(Salary)))
```

Best Subset regression
----------------------

The package `leaps` contains a command to evaluate all best-subset models.
```{r}
library(leaps)
regfit.full = regsubsets(Salary~., data = Hitters)
summary(regfit.full)
```

```{r}
regfit.full = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg.summary = summary(regfit.full)
names(reg.summary)
```

```{r}
plot(reg.summary$cp, xlab = "Number of variables", ylab = "Cp")
minIndex = which.min(reg.summary$cp)
points(10, reg.summary$cp[minIndex], pch = 20, col = "red")
```

```{r}
plot(regfit.full, scale = "Cp")
coef(regfit.full, 10)
```

Forward Stepwise Selection
--------------------------

`regsubsets` also supports forward stepwise selection.

```{r}
regfit.fwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
plot(regfit.fwd, scale = "Cp")
```

Of course a subset of the data can be used.

```{r}
dim(Hitters)
set.seed(1)
train = sample(seq(263), 180, replace = FALSE)
train
regfit.fwd = regsubsets(Salary ~ ., data = Hitters[train,], nvmax = 19, method = "forward")
```

Now iterate all 19 models and test them on the test data set (= the data points not used in training).

```{r}
val.errors = rep(NA, 19)
x.test = model.matrix(Salary ~ ., data = Hitters[-train,]) # -train *excludes* observation by train

for (i in 1:19) {
  # the used variables (coef)
  coefi = coef(regfit.fwd, id = i)
  # select the features and multiply with their factors given by the current model
  pred = x.test[, names(coefi)] %*% coefi
  # calculate RSS
  val.errors[i] = mean((Hitters$Salary[-train]-pred)^2)
}
```

Now plot the results.

```{r}
plot(sqrt(val.errors), ylab = "Root MSE", ylim = c(300,400), pch = 19, type = "b")
points(sqrt(regfit.fwd$rss[-1]/180), col = "blue", pch = 19, type = "b")
legend("topright", legend = c("Training", "Validation"), col = c("blue", "black"), pch = 19)
```

It looks like the model with 5 features selected by forward selection performs best. There is a noteworthy jump of validation error at index 7 which jumps back. From index 14 on the model overfits immensely.

Instead of hacking prediction with regsubsets it should be factored in an own method:

```{r}
predict.regsubsets = function(object, newdata, id, ...) {
  # extract the formula from the model
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  mat[,names(coefi)] %*% coefi
}
```

Model selection by Cross-Validation
----------------------------------

```{r}
set.seed(2)
folds = sample(rep(1:10, length = nrow(Hitters)))
folds
table(folds)
```

```{r}
cv.errors = matrix(NA, 10, 19)

for (k in 1:10) {
  best.fit = regsubsets(Salary ~ ., data = Hitters[folds!=k,], nvmax = 19, method = "forward")
  for (i in 1:19) {
    pred = predict(best.fit, Hitters[folds==k,], id = i)
    cv.errors[k,i] = mean( (Hitters$Salary[folds == k] - pred)^2)
  }
}

rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch = 19, type = "b")
```

Ridge Regression and the Lasso
------------------------------

The `glmnet` package supplies some models such as ridge and lasse regression, note that it does not conform to the regular model fitting notation.

```{r}
library(glmnet)
x = model.matrix(Salary ~ .-1, data = Hitters)
y = Hitters$Salary
```

When glmnet is being called with $\alpha = 0$, then ridge regression is run.

```{r}
fit.ridge = glmnet(x, y, alpha = 0)
plot(fit.ridge, xvar = "lambda", label = TRUE)
```

As it can be seen the coefficients converge to zero for huge value of lambda.

With cross-validation the test error can be estimate.
```{r}
cv.ridge = cv.glmnet(x, y, alpha = 0)
plot(cv.ridge)
```

The two vertical lines are at the minimum and one std error away from the minimum (a good position to pick lambda from).

To fit a lasso model $\alpha = 1$ should be used.

```{r}
fit.lasso = glmnet(x,y, alpha = 1)
plot(fit.lasso, xvar = 'lambda', label = TRUE)
```

It can be seen that lasso regression pushes coefficients to zero much more aggressive. So the desired property of lasso regression shows here quite well.

```{r}
cv.lasso = cv.glmnet(x, y)
plot(cv.lasso)
coef(cv.lasso)
```

```{r}
lasso.tr = glmnet(x[train,], y[train])
lasso.tr
pred = predict(lasso.tr, x[-train,])
dim(pred)

# build the validation curve by lambda
rmse = sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda), rmse, type = "b", xlba = "log(lambda)")
lam.best = lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr, s = lam.best)
```


